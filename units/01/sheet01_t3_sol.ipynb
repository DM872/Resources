{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "The solution proposed here is partly taken from Gurobi [Feature Selection case](https://colab.research.google.com/github/Gurobi/modeling-examples/blob/master/linear_regression/l0_regression.ipynb) to which you are referred for a more thorough machine learning pipeline. Here we focus only on the optimization problems.\n",
    "\n",
    "## Solution Approach\n",
    "\n",
    "\n",
    "\n",
    "### Sets and Indices\n",
    "\n",
    "$i \\in I=\\{1,2,\\dots,n\\}$: Set of observations.\n",
    "\n",
    "$j \\in J=\\{0,1,2,\\dots,p\\}$: Set of features, where the first ID corresponds to the intercept.\n",
    "\n",
    "$\\ell \\in L = J \\setminus \\{0\\}$: Set of features, where the intercept is excluded.\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "$s \\in \\mathbb{N}$: Number of features to include in the model, ignoring the intercept.\n",
    "\n",
    "\n",
    "### Decision Variables\n",
    "\n",
    "$\\beta_j \\in \\mathbb{R}$: Weight of feature $j$, representing the change in the response variable per unit-change of feature $j$.\n",
    "\n",
    "$z_\\ell \\in \\{0,1\\}$: 1 if weight of feature $\\ell$ is exactly equal to zero, and 0 otherwise. Auxiliary variable used to manage the budget constraint.\n",
    "\n",
    "\n",
    "### Training error\n",
    "\n",
    "The training error is captured by a loss function, which can be defined as the Sum of Absolute Residuals (aka L1):\n",
    "\n",
    "\\begin{equation*}\n",
    " Z = \\sum_{i \\in I}\\left|y_i-\\sum_{j \\in J}\\beta_{j}x_{ij}\\right|\n",
    "\\end{equation*}\n",
    "\n",
    "or by the Residual Sum of Squares (RSS) (aka L2):\n",
    "\n",
    "\\begin{equation*}\n",
    " Z = \\sum_{i \\in I}\\left(y_i-\\sum_{j \\in\n",
    "J}\\beta_{j}x_{ij}\\right)^2 = \\beta^T X^T X\\beta- 2y^TX\\beta+y^T y\n",
    "\\end{equation*}\n",
    "\n",
    "Minimizing the loss function Z corresponds to solving an unconstrained optimization problem. The RSS formulation is the standard technique because it admits a closed form solution, which is given in the task statement and known as Ordinary Least Squares.\n",
    "The Sum of Aboslute Errors is more robust with respect to outliers but it does not have a closed form solution mainly because the absolute error is not differentiable. However, it can be solved by Linear Programming.\n",
    "\n",
    "\n",
    "### Feature selection\n",
    "\n",
    "One way to set the $\\beta$ terms to zero is to weight them in the objective function. This is what the Lasso approach does either using the L1 form:\n",
    "\n",
    "\\begin{equation*}\n",
    "L=  \\sum_{\\ell=1}^p\\left|\\beta_\\ell\\right|\n",
    "\\end{equation*}\n",
    "\n",
    "or the L2 form: \n",
    "\n",
    "\\begin{equation*}\n",
    "L= \\sum_{\\ell=1}^p\\left(\\beta_\\ell\\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "This leads to solving another unconstrained optimization problem, namely:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\min Z + \\lambda L\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\lambda$ is a parameter that weights the importance of the Lasso term.\n",
    "With the Lasso term also the RSS formalization of the training error becomes not solvable in closed form. However, the function is still convex and solved efficiently by (stochastic) gradient descent algorithms. This approach is used also in Deep Learning where the Lasso term is known as regularization term.\n",
    "\n",
    "\n",
    "Alternatively, advances in mixed integer linear programming made it possible to model the feature selection more directly and precisely by means of constraints. \n",
    "\n",
    "Let us introduce binary variables $z_\\ell$. They can be used to determine the features that are discarded from the regression: for each feature $\\ell$, if $z_\\ell=1$, then $\\beta_\\ell=0$. These are mutual exclusivity kind of constraints that Gurobi can handle in a specialized manner. In the solution provided by Gurobi this way of modeling the feature selection is called the $L_0$-norm and Gurobi provides modern [General Constraint Helper Functions](https://docs.gurobi.com/projects/optimizer/en/current/reference/python/model.html#Model.addGenConstrNorm) to implement them. Here, however, we go the traditional way and linearize the mutal exclusivity constraints as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left|\\beta_\\ell\\right|\\leq M (1-z_\\ell)\n",
    "\\end{equation*}\n",
    "Here the big-M is the upper bound of the $\\beta$ variables, which are actually\n",
    "unlimited. \n",
    "\n",
    "We can express each of these $|L|$ constraints as a [Special Ordered Set of type 1](https://docs.gurobi.com/projects/optimizer/en/current/concepts/modeling/constraints.html#subsubsectionsosconstraints), meaning that at most one variable is different from zero (variables can be both integer or continuous in SOS-1):\n",
    "\n",
    "\\begin{equation*}\n",
    "(\\beta_\\ell, z_\\ell): \\text{SOS-1} \\quad \\forall \\ell \\in L\n",
    "\\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "This simply leaves to the solver the task of writing the M-constraints above.\n",
    "\n",
    "Finally, in both cases we have the budget constraints satating that, exactly, $|L| - s$ feature coefficients must be equal to zero:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{\\ell \\in L}z_\\ell = |L| - s\n",
    "\\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "This model, by means of constraint 2, implicitly considers all ${{p} \\choose s}$ feature subsets at once. However, we also need to find the value for $s$ that maximizes the performance of the regression on unseen observations. Notice that the training error decreases monotonically as more features are considered, so it is not advisable to use it as the performance metric. Instead, we should estimate the Mean Squared Error (MSE) via cross-validation. This metric is defined as $\\text{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}{(y_i-\\hat{y}_i)^2}$, where $y_i$ and $\\hat{y}_i$ are the observed and predicted values for the ith observation, respectively. Then, we will fine-tune $s$ using grid search, provided that the set of possible values is quite small.\n",
    "\n",
    "\n",
    "### Linearization\n",
    "\n",
    "All functions used above that have aboslute value terms can be linearized as follows. \n",
    "\n",
    "The objective function $\\min Z =\\sum_{i \\in I}\\left|y_i-\\sum_{j \\in J}\\beta_{j}x_{ij}\\right|$ becomes\n",
    "\n",
    "\\begin{align*}\n",
    "\\min &\\sum_{i \\in I} \\epsilon_i^+ +\\epsilon_i^-\\\\\n",
    " &y_i-\\sum_{j \\in J}\\beta_{j}x_{ij} = \\epsilon_i^+ -\\epsilon_i^-\\\\\n",
    " &\\epsilon_i^+, \\epsilon_i^- \\geq 0\n",
    "\\end{align*}\n",
    "\n",
    "and the constraints $\\left|\\beta_\\ell\\right|\\leq M (1-z_\\ell)$ for all $\\ell \\in L$ become:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\beta_\\ell^+ +\\beta_\\ell^- \\leq M(1-z_\\ell)\\\\\n",
    " &\\beta_{\\ell} = \\beta_\\ell^+ -\\beta_\\ell^-\\\\\n",
    " &\\beta_\\ell^+, \\beta_\\ell^- \\geq 0\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Hence, the overall regression problem with Z formulated with L1 and feature selection formulated as constraints becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min &\\sum_{i \\in I} (\\epsilon_i^+ +\\epsilon_i^-)\\\\\n",
    " &y_i-\\sum_{j \\in J}\\beta_{j}x_{ij} = \\epsilon_i^+ -\\epsilon_i^- \\quad \\text{ for all } i \\in I\\\\\n",
    "&\\beta_\\ell^+ +\\beta_\\ell^- \\leq M(1-z_\\ell)\\quad \\text{ for all }\\ell \\in L\\\\\n",
    " &\\beta_{\\ell} = \\beta_\\ell^+ -\\beta_\\ell^- \\quad \\text{ for all } \\ell \\in L\\\\\n",
    " &\\sum_{\\ell \\in L}z_\\ell = |L| - s\\\\\n",
    " &\\epsilon_i^+, \\epsilon_i^- \\geq 0 \\quad \\text{ for all }i \\in I\\\\\n",
    " &\\beta_\\ell^+, \\beta_\\ell^- \\geq 0 \\quad \\text{ for all } \\ell \\in L\\\\\n",
    " &\\beta_j \\in \\mathbb{R} \\quad \\text{ for all } j \\in J\\\\\n",
    " &z_\\ell \\in \\{0,1\\}\\quad \\quad \\text{ for all } \\ell \\in L\n",
    "\\end{align*}\n",
    "This is a mixed integer linear programming problem.\n",
    "With Gurobi it is however also possible to solve the problem with a quadratic objecitve function as with the RSS formulation. Moreover, it is possible to specify the problem using matrices and vectors rather than scalars as shown here. See the solutions by Gurobi listed at the beginnning of this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Implementation\n",
    "\n",
    "Here we show the implementation of the mixed integer linear programming problem formulated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and split into train (80%) and test (20%)\n",
    "housing = fetch_california_housing()\n",
    "X = housing['data']\n",
    "y = housing['target']\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2,random_state=10101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain_std = scaler.transform(Xtrain)\n",
    "Xtest_std = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_std.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This function assumes the design matrix features does not contain\n",
    "#a column for the intercept\n",
    "def milp(features, response, non_zero, sos=False, verbose=False):\n",
    "    m = gp.Model(\"fitting\")\n",
    "    m.setParam(\"FeasibilityTol\",1e-9)\n",
    "    m.setParam(\"OptimalityTol\",1e-9)\n",
    "    m.setParam(\"IntFeasTol\",1e-9)\n",
    "\n",
    "    # Sets\n",
    "    p = features.shape[1] # n of features\n",
    "    n = features.shape[0] # n of observations (data points)\n",
    "\n",
    "    I=range(n)\n",
    "    J=range(p+1)\n",
    "    L=range(1,p+1)\n",
    "\n",
    "    # Create the decision variables\n",
    "    beta = m.addVars(J,lb=-float('inf'), vtype=GRB.CONTINUOUS, name=f\"beta_\")\n",
    "    zeta = m.addVars(L, vtype=GRB.BINARY, name=\"zeta_\")\n",
    "\n",
    "    # auxiliary to handle absolute values\n",
    "    betap = m.addVars(L, lb=0.0, vtype=GRB.CONTINUOUS, name=\"betap_\")\n",
    "    betan = m.addVars(L, lb=0.0, vtype=GRB.CONTINUOUS, name=\"betan_\")\n",
    "            \n",
    "    errp = m.addVars(I, lb=0.0, vtype=GRB.CONTINUOUS, name=\"errn_\")\n",
    "    errn = m.addVars(I, lb=0.0, vtype=GRB.CONTINUOUS, name=\"errp_\")\n",
    "\n",
    "\n",
    "    # The objective is to minimize \n",
    "    m.modelSense=gp.GRB.MINIMIZE\n",
    "    m.setObjective(gp.quicksum(errp[i]+errn[i] for i in I))\n",
    "\n",
    "    for i in I:\n",
    "        m.addConstr(errp[i]-errn[i]==response[i]-beta[0]-gp.quicksum(beta[j]*features[i][j-1] for j in L ),name=f\"loss_{i}\")\n",
    "\n",
    "    # Mutual exclusivity\n",
    "    # Parameter\n",
    "    if not sos:\n",
    "        M=1e9 #float('inf')    \n",
    "        for j in L:\n",
    "            m.addConstr(betap[j]-betan[j]==beta[j],name=f\"cbeta_{j}\")\n",
    "            m.addConstr(betap[j]+betan[j]<=M*(1-zeta[j]),name=f\"M-constr_{j}\")\n",
    "    else:\n",
    "        for j in L:\n",
    "            # If zeta[i]=1, then beta[i] = 0\n",
    "            m.addSOS(GRB.SOS_TYPE1, [zeta[j], beta[j]])\n",
    "        \n",
    "    # Budget constraints\n",
    "    m.addConstr(gp.quicksum(zeta[j] for j in L)>=p-non_zero, name=\"budget\")    \n",
    "\n",
    "    # Solve\n",
    "    m.write(\"feat_sel.lp\")\n",
    "    m.optimize()\n",
    "\n",
    "    if m.status == gp.GRB.status.OPTIMAL:\n",
    "        print('\\nSum of Absolute Residuals (Training Error): %g' % m.ObjVal)\n",
    "        print('\\nbetas:')\n",
    "        for j in J:\n",
    "            #if math.fabs(beta[j].X)>0+0.0001:\n",
    "            print('%s %g' % (j, beta[j].X))\n",
    "        print('\\nzetas:')\n",
    "        for ell in L:\n",
    "            #if zeta[j].X>0+0.0001:\n",
    "            print('%s %g' % (ell, zeta[ell].X))\n",
    "    else:\n",
    "        print('No solution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter FeasibilityTol to value 1e-09\n",
      "Set parameter OptimalityTol to value 1e-09\n",
      "Set parameter IntFeasTol to value 1e-09\n",
      "Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (mac64[arm] - Darwin 24.6.0 24G419)\n",
      "\n",
      "CPU model: Apple M1 Max\n",
      "Thread count: 10 physical cores, 10 logical processors, using up to 10 threads\n",
      "\n",
      "Non-default parameters:\n",
      "FeasibilityTol  1e-09\n",
      "IntFeasTol  1e-09\n",
      "OptimalityTol  1e-09\n",
      "\n",
      "Optimize a model with 16529 rows, 33057 columns and 181688 nonzeros (Min)\n",
      "Model fingerprint: 0x7ef025cc\n",
      "Model has 33024 linear objective coefficients\n",
      "Variable types: 33049 continuous, 8 integer (8 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [9e-06, 1e+09]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e-01, 1e+09]\n",
      "Warning: Model contains large matrix coefficient range\n",
      "         Consider reformulating model or setting NumericFocus parameter\n",
      "         to avoid numerical issues.\n",
      "\n",
      "Presolve time: 0.10s\n",
      "Presolved: 16529 rows, 33057 columns, 181688 nonzeros\n",
      "Variable types: 33049 continuous, 8 integer (8 binary)\n",
      "\n",
      "Deterministic concurrent LP optimizer: primal and dual simplex (primal and dual model)\n",
      "Showing primal log only...\n",
      "\n",
      "Root relaxation presolved: 16529 rows, 33057 columns, 181688 nonzeros\n",
      "\n",
      "Concurrent spin time: 0.05s\n",
      "\n",
      "Solved with dual simplex\n",
      "\n",
      "Root relaxation: objective 8.493985e+03, 111 iterations, 0.15 seconds (0.15 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 8493.98474    0    1          - 8493.98474      -     -    0s\n",
      "     0     0 8493.98474    0    1          - 8493.98474      -     -    0s\n",
      "     0     2 8493.98474    0    1          - 8493.98474      -     -    0s\n",
      "*    2     2               1    8692.4082895 8493.98474  2.28%   816    0s\n",
      "*    8     4               3    8642.4553479 8493.98474  1.72%   445    1s\n",
      "*   12     4               4    8547.4095386 8493.98474  0.63%   383    1s\n",
      "\n",
      "Explored 20 nodes (6053 simplex iterations) in 2.04 seconds (6.28 work units)\n",
      "Thread count was 10 (of 10 available processors)\n",
      "\n",
      "Solution count 3: 8547.41 8642.46 8692.41 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 8.547409538647e+03, best bound 8.547409538647e+03, gap 0.0000%\n",
      "\n",
      "Sum of Absolute Residuals (Training Error): 8547.41\n",
      "\n",
      "betas:\n",
      "0 1.9493\n",
      "1 0.926374\n",
      "2 0.0799507\n",
      "3 -0.404916\n",
      "4 0.487348\n",
      "5 0\n",
      "6 0\n",
      "7 -0.759661\n",
      "8 -0.763164\n",
      "\n",
      "zetas:\n",
      "1 1\n",
      "2 9.25124e-10\n",
      "3 -0\n",
      "4 -0\n",
      "5 1\n",
      "6 1\n",
      "7 -0\n",
      "8 -0\n"
     ]
    }
   ],
   "source": [
    "milp(Xtrain_std, ytrain, 5, True) # 5 features in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is awkward. Rerunning using the SOS-2 constraints we obtain the\n",
    "correct result.\n",
    "\n",
    "-- \n",
    "\n",
    "The quadratic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This function assumes the design matrix features does not contain\n",
    "#a column for the intercept\n",
    "def miqp(features, response, non_zero, verbose=False):\n",
    "    \"\"\"\n",
    "    Deploy and optimize the MIQP formulation of L0-Regression.\n",
    "    \"\"\"\n",
    "    assert isinstance(non_zero, (int, np.integer))\n",
    "    # Create a Gurobi environment and a model object\n",
    "    with gp.Env() as env, gp.Model(\"\", env=env) as regressor:\n",
    "        samples, dim = features.shape\n",
    "        assert samples == response.shape[0]\n",
    "        assert non_zero <= dim\n",
    "\n",
    "        # Append a column of ones to the feature matrix to account for the y-intercept\n",
    "        X = np.concatenate([features, np.ones((samples, 1))], axis=1)\n",
    "\n",
    "        # Decision variables\n",
    "        norm_0 = regressor.addVar(lb=non_zero, ub=non_zero, name=\"norm\")\n",
    "        beta = regressor.addMVar((dim + 1,), lb=-GRB.INFINITY, name=\"beta\") # Weights\n",
    "        intercept = beta[dim] # Last decision variable captures the y-intercept\n",
    "\n",
    "        regressor.setObjective(beta.T @ X.T @ X @ beta\n",
    "                               - 2*response.T @ X @ beta\n",
    "                               + np.dot(response, response), GRB.MINIMIZE)\n",
    "\n",
    "        # Budget constraint based on the L0-norm\n",
    "        regressor.addGenConstrNorm(norm_0, beta[:-1], which=0, name=\"budget\")\n",
    "\n",
    "        if not verbose:\n",
    "            regressor.params.OutputFlag = 0\n",
    "        regressor.params.timelimit = 60\n",
    "        regressor.params.mipgap = 0.001\n",
    "        regressor.optimize()\n",
    "\n",
    "        coeff = np.array([beta[i].X for i in range(dim)])\n",
    "        return intercept.X, coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Set parameter LicenseID to value 2762533\n",
      "Academic license - for non-commercial use only - expires 2027-01-09\n",
      "Set parameter TimeLimit to value 60\n",
      "Set parameter MIPGap to value 0.001\n",
      "Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (mac64[arm] - Darwin 24.6.0 24G419)\n",
      "\n",
      "CPU model: Apple M1 Max\n",
      "Thread count: 10 physical cores, 10 logical processors, using up to 10 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  60\n",
      "MIPGap  0.001\n",
      "\n",
      "Optimize a model with 0 rows, 10 columns and 0 nonzeros (Min)\n",
      "Model fingerprint: 0xa6f503be\n",
      "Model has 9 linear objective coefficients and an objective constant of 9.3360800554079950e+04\n",
      "Model has 45 quadratic objective terms\n",
      "Model has 1 simple general constraint\n",
      "  1 NORM\n",
      "Variable types: 10 continuous, 0 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [0e+00, 0e+00]\n",
      "  Objective range  [9e+02, 7e+04]\n",
      "  QObjective range [1e-12, 6e+04]\n",
      "  Bounds range     [5e+00, 5e+00]\n",
      "  RHS range        [0e+00, 0e+00]\n",
      "Warning: Model contains large quadratic objective coefficient range\n",
      "         Consider reformulating model or setting NumericFocus parameter\n",
      "         to avoid numerical issues.\n",
      "\n",
      "Warning: Greater-than sense for 0-norm general constraint may produce\n",
      "         unexpected results\n",
      "Presolve added 17 rows and 23 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 17 rows, 33 columns, 56 nonzeros\n",
      "Presolved model has 16 SOS constraint(s)\n",
      "Presolved model has 45 quadratic objective terms\n",
      "Variable types: 25 continuous, 8 integer (8 binary)\n",
      "\n",
      "Root relaxation: objective 8.730267e+03, 33 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 8730.26692    0    3          - 8730.26692      -     -    0s\n",
      "H    0     0                    21929.176826 8730.26692  60.2%     -    0s\n",
      "H    0     0                    18635.278846 8730.26692  53.2%     -    0s\n",
      "     0     0 8730.26692    0    3 18635.2788 8730.26692  53.2%     -    0s\n",
      "     0     2 8730.26692    0    3 18635.2788 8730.26692  53.2%     -    0s\n",
      "H    3     8                    11419.145619 8730.26692  23.5%   4.3    0s\n",
      "H    5     7                    10824.852224 8730.26692  19.3%   3.8    0s\n",
      "H    7     8                    9165.4763890 8730.26692  4.75%   5.0    0s\n",
      "*   18     6               5    8892.1358039 8730.26692  1.82%   3.4    0s\n",
      "\n",
      "Explored 22 nodes (110 simplex iterations) in 0.02 seconds (0.00 work units)\n",
      "Thread count was 10 (of 10 available processors)\n",
      "\n",
      "Solution count 6: 8892.14 9165.48 10824.9 ... 21929.2\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-03)\n",
      "Best objective 8.892135803895e+03, best bound 8.892135803895e+03, gap 0.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(2.07596342),\n",
       " array([ 0.72353363,  0.12587989,  0.        ,  0.07955282,  0.        ,\n",
       "         0.        , -0.99118496, -0.94202137]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "miqp(Xtrain_std, ytrain, 5, True) # 5 features in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That shows the intercept and the feature coefficients.\n",
    "\n",
    "We refer to Gurobi solution for a comparison of the models' performance on the\n",
    "housing learning task. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
